{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('C:\\\\Users\\\\SANHA\\\\Desktop\\\\new_data.csv', delimiter=',', dtype=np.float32)\n",
    "x_data=xy[:,0:-1]\n",
    "x_data = normalize(x_data, axis=0, norm='max')\n",
    "y_data=xy[:,[-1]]\n",
    "m_data=xy[:499,:]\n",
    "r_data=xy[:1000,:]\n",
    "g_data=xy[:0,:]\n",
    "F_data=xy[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inputs\n",
    "def model_inputs(real_dim, noise_dim):\n",
    "    inputs_real_ = tf.placeholder(tf.float32, shape=[None, real_dim], name='inputs_real')\n",
    "    inputs_z_ = tf.placeholder(tf.float32, shape=[None, noise_dim], name='inputs_z')\n",
    "    \n",
    "    return inputs_real_, inputs_z_\n",
    "\n",
    "#def leaky_relu(x, alpha):\n",
    " #   return tf.maximum(alpha * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Network# Genera \n",
    "def model_generator(z_input, out_dim, n_units=128, reuse=False, alpha=0.01):\n",
    "    # used to reuse variables, name scope\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        hidden_layer = tf.layers.dense(z_input, n_units, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden_layer, out_dim, activation=None)\n",
    "        outputs = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return outputs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Network\n",
    "def model_discriminator(input, n_units=128, reuse=False, alpha=0.1):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        hidden_layer = tf.layers.dense(input, n_units, activation=tf.nn.relu)\n",
    "        #hidden_layer = leaky_relu(hidden_layer, alpha)\n",
    "        \n",
    "        logits = tf.layers.dense(hidden_layer, 1, activation=None\n",
    "                                )\n",
    "        outputs = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return outputs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 42\n",
    "z_dim = 21\n",
    "g_hidden_size = 128\n",
    "d_hidden_size = 128\n",
    "alpha = 0.1\n",
    "smooth = 0.1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()  # If we don't have this, as we call this block over and over, the graph gets bigger and bigger\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs_real, inputs_z = model_inputs(input_size, z_dim)\n",
    "    \n",
    "    g_outputs, g_logits = model_generator(inputs_z, input_size, n_units=g_hidden_size, reuse=False, alpha=alpha)\n",
    "    \n",
    "    d_outputs_real, d_logits_real = model_discriminator(inputs_real, n_units=d_hidden_size, reuse=False, alpha=alpha)\n",
    "    d_outputs_fake, d_logits_fake = model_discriminator(g_outputs, n_units=d_hidden_size, reuse=True, alpha=alpha)\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_logits_real) * (1-smooth)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)))\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)))\n",
    "    \n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [variable for variable in t_vars if 'generator' in variable.name]\n",
    "    d_vars = [variable for variable in t_vars if 'discriminator' in variable.name]\n",
    "    \n",
    "    # Affected Variables with var_list\n",
    "    d_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "    g_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "    \n",
    "    # Saving variables with var_list\n",
    "    saver = tf.train.Saver(var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 42)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 / 10000 Complete. D_Loss : 1.573, G_Loss : 0.874\n",
      "step 101 / 10000 Complete. D_Loss : 2.088, G_Loss : 0.973\n",
      "step 201 / 10000 Complete. D_Loss : 1.848, G_Loss : 1.134\n",
      "step 301 / 10000 Complete. D_Loss : 5.084, G_Loss : 1.601\n",
      "step 401 / 10000 Complete. D_Loss : 9.643, G_Loss : 1.571\n",
      "Training Complete. \n"
     ]
    }
   ],
   "source": [
    "samples=[]\n",
    "#line_sample=[1][42]\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    f = open('C:\\\\Users\\\\SANHA\\\\Desktop\\\\gen_sample.txt', 'w')\n",
    "    for step in range(500):\n",
    "        \n",
    "        \n",
    "        batch_images = F_data[step].reshape([1, 42])\n",
    "        batch_z = np.random.uniform(0, 1, size=[1, z_dim])\n",
    "        \n",
    "        _ = sess.run(d_optimizer, feed_dict={inputs_real : batch_images, inputs_z : batch_z})\n",
    "        _ = sess.run(g_optimizer, feed_dict={inputs_z : batch_z})\n",
    "        loss_d, loss_g = sess.run([d_loss, g_loss], feed_dict={inputs_real : batch_images, inputs_z : batch_z})\n",
    "        if step%100==0:\n",
    "            print('step {} / {} Complete. D_Loss : {:0.3f}, G_Loss : {:0.3f}'.format(step+1, 10000, loss_d, loss_g))\n",
    "        \n",
    "        sample_z = np.random.uniform(0, 1, size=[1, z_dim])  # 16 Samples each epoch\n",
    "        gen_samples, _ = sess.run(model_generator(inputs_z, input_size, reuse=True), feed_dict={inputs_z : sample_z})\n",
    "        f.writelines((str(gen_samples)))\n",
    "        temp=gen_samples[0,41]\n",
    "        if temp>=0.5:\n",
    "            gen_samples[0,41]=1\n",
    "        else :\n",
    "            gen_samples[0,41]=0\n",
    "        f.writelines((str(gen_samples)))\n",
    "        m_data=np.append(m_data,gen_samples,axis=0)\n",
    "        g_data=np.append(g_data,gen_samples,axis=0)\n",
    "       \n",
    "        \n",
    "    \n",
    "    print('Training Complete. ')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1009, 42)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:     0\tLoss: 2.885\tAcc: 25.55%\n",
      "step:    10\tLoss: 1.679\tAcc: 32.22%\n",
      "step:    20\tLoss: 0.978\tAcc: 59.07%\n",
      "step:    30\tLoss: 0.685\tAcc: 76.56%\n",
      "step:    40\tLoss: 0.556\tAcc: 82.32%\n",
      "step:    50\tLoss: 0.488\tAcc: 85.60%\n",
      "step:    60\tLoss: 0.447\tAcc: 87.62%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-221-0626354a1edc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;31m#print(\"training by gan sample\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#xy = np.loadtxt('C:\\\\Users\\\\SANHA\\\\Desktop\\\\test.csv', delimiter=',', dtype=np.float32)\n",
    "#x_data=xy[:,0:-1]\n",
    "#y_data=xy[:,[-1]]\n",
    "#F_data=xy[:,:]\n",
    "\n",
    "\n",
    "x1_data=m_data[:,0:-1]\n",
    "y1_data=m_data[:,[-1]]\n",
    "x2_data=r_data[:,0:-1]\n",
    "y2_data=r_data[:,[-1]]\n",
    "x3_data=g_data[:,0:-1]\n",
    "y3_data=g_data[:,[-1]]\n",
    "\n",
    "\n",
    "\n",
    "nb_classes=2\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,41])\n",
    "Y=tf.placeholder(tf.int32,[None,1])\n",
    "\n",
    "Y_one_hot=tf.one_hot(Y,nb_classes)\n",
    "Y_one_hot=tf.reshape(Y_one_hot,[-1,nb_classes])\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([41,128]),name='weight1')\n",
    "b1=tf.Variable(tf.random_normal([128]),name='bias1')\n",
    "layer1=tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([128,128]),name='weight2')\n",
    "b2=tf.Variable(tf.random_normal([128]),name='bias2')\n",
    "layer2=tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3=tf.Variable(tf.random_normal([128,nb_classes]),name='weight3')\n",
    "b3=tf.Variable(tf.random_normal([nb_classes]),name='bias3')\n",
    "logits=tf.matmul(layer2,W3)+b3\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y_one_hot)\n",
    "\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "prediction=tf.argmax(hypothesis,1) #가능성을 퍼센트로~~\n",
    "correct_prediction=tf.equal(prediction,tf.arg_max(Y_one_hot,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        sess.run(optimizer,feed_dict={X:x_data,Y:y_data})\n",
    "        #print(\"training by gan sample\")\n",
    "        if step %10==0:\n",
    "            loss,acc=sess.run([cost,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "            print(\"step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step,loss,acc))\n",
    "  \n",
    "    tr=0\n",
    "    fa=0\n",
    "    total=0\n",
    "\n",
    "\n",
    "    pred = sess.run(prediction, feed_dict={X: x1_data})\n",
    "    for p, y in zip(pred, y1_data.flatten()):\n",
    "            if(p==int(y)):\n",
    "                tr=tr+1\n",
    "            else:\n",
    "                fa=fa+1\n",
    "            #print(\"[{}] Prediction: {} Real Y: {}\".format(p == int(y), p, int(y)))\n",
    "\n",
    "    print(\"true={} false: {} acc: {:0.2f}%\".format(tr,fa,tr/(tr+fa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
