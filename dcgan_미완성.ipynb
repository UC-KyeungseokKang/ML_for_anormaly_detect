{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline \n",
    "import sys\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('C:\\\\Users\\\\SANHA\\\\Desktop\\\\new_data.csv', delimiter=',', dtype=np.float32)\n",
    "x_data=xy[:,0:-1]\n",
    "x_data = normalize(x_data, axis=0, norm='max')\n",
    "y_data=xy[:,[-1]]\n",
    "m_data=xy[:499,:]\n",
    "\n",
    "F_data=xy[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples, D_loss, G_loss, epoch, total):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))        \n",
    "\n",
    "    gs = gridspec.GridSpec(4, 8)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    \n",
    "    # Plot losses\n",
    "    ax = plt.subplot(gs[:, 4:])\n",
    "    ax.plot(D_loss, label=\"discriminator's loss\", color='b')\n",
    "    ax.plot(G_loss, label=\"generator's loss\", color='r')\n",
    "    ax.set_xlim([0, total])\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.legend()\n",
    "    for i, sample in enumerate(samples):\n",
    "\n",
    "        if i > 4* 4 - 1:\n",
    "            break\n",
    "        ax = plt.subplot(gs[i % 4, int(i / 4)])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    plt.savefig('./output/' + str(epoch + 1) + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Generate images\n",
    "    \n",
    "\n",
    "def Conv2d(input, output_dim=64, kernel=(5, 5), strides=(2, 2), stddev=0.2, name='conv_2d'):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('Conv2dW', [kernel[0], kernel[1], input.get_shape()[-1], output_dim],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        b = tf.get_variable('Conv2db', [output_dim], initializer=tf.zeros_initializer())\n",
    "        \n",
    "        return tf.nn.conv2d(input, W, strides=[1, strides[0], strides[1], 1], padding='SAME') + b\n",
    "\n",
    "def Deconv2d(input, output_dim, batch_size, kernel=(5, 5), strides=(2, 2), stddev=0.2, name='deconv_2d'):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('Deconv2dW', [kernel[0], kernel[1], output_dim, input.get_shape()[-1]],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        b = tf.get_variable('Deconv2db', [output_dim], initializer=tf.zeros_initializer())\n",
    "\n",
    "        input_shape = input.get_shape().as_list()\n",
    "        output_shape = [batch_size,\n",
    "                        int(input_shape[1] * strides[0]),\n",
    "                        int(input_shape[2] * strides[1]),\n",
    "                        output_dim]\n",
    "\n",
    "        deconv = tf.nn.conv2d_transpose(input, W, output_shape=output_shape,\n",
    "                                        strides=[1, strides[0], strides[1], 1])\n",
    "    \n",
    "        return deconv + b\n",
    "\n",
    "def Dense(input, output_dim, stddev=0.02, name='dense'):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        shape = input.get_shape()\n",
    "        W = tf.get_variable('DenseW', [shape[1], output_dim], tf.float32,\n",
    "                            tf.random_normal_initializer(stddev=stddev))\n",
    "        b = tf.get_variable('Denseb', [output_dim],\n",
    "                            initializer=tf.zeros_initializer())\n",
    "        \n",
    "        return tf.matmul(input, W) + b\n",
    "\n",
    "def BatchNormalization(input, name='bn'):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "    \n",
    "        output_dim = input.get_shape()[-1]\n",
    "        beta = tf.get_variable('BnBeta', [output_dim],\n",
    "                            initializer=tf.zeros_initializer())\n",
    "        gamma = tf.get_variable('BnGamma', [output_dim],\n",
    "                            initializer=tf.ones_initializer())\n",
    "    \n",
    "        if len(input.get_shape()) == 2:\n",
    "            mean, var = tf.nn.moments(input, [0])\n",
    "        else:\n",
    "            mean, var = tf.nn.moments(input, [0, 1, 2])\n",
    "        return tf.nn.batch_normalization(input, mean, var, beta, gamma, 1e-5)\n",
    "    \n",
    "def LeakyReLU(input, leak=0.2, name='lrelu'):\n",
    "    \n",
    "    return tf.maximum(input, leak*input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(X, reuse=False, name='d'):\n",
    "\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "\n",
    "        if len(X.get_shape()) > 2:\n",
    "            # X: -1, 28, 28, 1\n",
    "            D_conv1 = Conv2d(X, output_dim=64, name='D_conv1')\n",
    "        else:\n",
    "            D_reshaped = tf.reshape(X, [-1, 1, 42, 1])\n",
    "            D_conv1 = Conv2d(D_reshaped, output_dim=64, name='D_conv1')\n",
    "        D_h1 = LeakyReLU(D_conv1) # [-1, 28, 28, 64]\n",
    "        D_conv2 = Conv2d(D_h1, output_dim=128, name='D_conv2')\n",
    "        D_h2 = LeakyReLU(D_conv2) # [-1, 28, 28, 128]\n",
    "        D_r2 = tf.reshape(D_h2, [-1, 256])\n",
    "        D_h3 = LeakyReLU(D_r2) # [-1, 256]\n",
    "        D_h4 = tf.nn.dropout(D_h3, 0.5)\n",
    "        D_h5 = Dense(D_h4, output_dim=1, name='D_h5') # [-1, 1]\n",
    "        return tf.nn.sigmoid(D_h5)\n",
    "\n",
    "def Generator(z, name='g'):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        G_1 = Dense(z, output_dim=1024, name='G_1') # [-1, 1024]\n",
    "        G_bn1 = BatchNormalization(G_1, name='G_bn1')\n",
    "        G_h1 = tf.nn.relu(G_bn1)\n",
    "        G_2 = Dense(G_h1, output_dim=7*7*128, name='G_2') # [-1, 7*7*128]\n",
    "        G_bn2 = BatchNormalization(G_2, name='G_bn2')        \n",
    "        G_h2 = tf.nn.relu(G_bn2)\n",
    "        G_r2 = tf.reshape(G_h2, [-1, 7, 7, 128])\n",
    "        G_conv3 = Deconv2d(G_r2, output_dim=64, batch_size=1, name='G_conv3')\n",
    "        G_bn3 = BatchNormalization(G_conv3, name='G_bn3')        \n",
    "        G_h3 = tf.nn.relu(G_bn3)\n",
    "        G_conv4 = Deconv2d(G_h3, output_dim=1, batch_size=1, name='G_conv4')\n",
    "        G_r4 = tf.reshape(G_conv4, [-1, 42])\n",
    "        return tf.nn.sigmoid(G_r4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable G/G_1/DenseW already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\SANHA\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\SANHA\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\SANHA\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-b5d0f8d77b83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'G'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mD_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mD_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-231-0ff5de988f6e>\u001b[0m in \u001b[0;36mGenerator\u001b[1;34m(z, name)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mG_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'G_1'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# [-1, 1024]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mG_bn1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'G_bn1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mG_h1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_bn1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-226-896d83ffcca5>\u001b[0m in \u001b[0;36mDense\u001b[1;34m(input, output_dim, stddev, name)\u001b[0m\n\u001b[0;32m     35\u001b[0m       \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m       W = tf.get_variable('DenseW', [shape[1], output_dim], tf.float32,\n\u001b[1;32m---> 37\u001b[1;33m                           tf.random_normal_initializer(stddev=stddev))\n\u001b[0m\u001b[0;32m     38\u001b[0m       b = tf.get_variable('Denseb', [output_dim],\n\u001b[0;32m     39\u001b[0m                           initializer=tf.zeros_initializer())\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1326\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1328\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1329\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1330\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1088\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 743\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    744\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable G/G_1/DenseW already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\SANHA\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\SANHA\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\SANHA\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 42])\n",
    "z = tf.placeholder(tf.float32, shape=[None, 21])\n",
    "\n",
    "G = Generator(z, 'G')\n",
    "D_real = Discriminator(X, False, 'D')\n",
    "D_fake = Discriminator(G, True, 'D')\n",
    "\n",
    "D_loss = -tf.reduce_mean(tf.log(D_real) - tf.log(D_fake)) # Train to judge if the data is real correctly\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake)) # Train to pass the discriminator as real data\n",
    "\n",
    "vars = tf.trainable_variables()\n",
    "d_params = [v for v in vars if v.name.startswith('D/')]\n",
    "g_params = [v for v in vars if v.name.startswith('G/')]\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.1).minimize(D_loss, var_list=d_params)\n",
    "G_solver = tf.train.AdamOptimizer(learning_rate=2e-4, beta1=0.3).minimize(G_loss, var_list=g_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 / 10000 Complete. D_Loss : 8.680, G_Loss : 2.124\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'uniform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-ca35777ade4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msample_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 16 Samples each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mgen_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_z\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minputs_z\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0msample_z\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mgen_dd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mgen_re\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_dd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'uniform'"
     ]
    }
   ],
   "source": [
    "samples\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    f = open('gen_sample.txt', 'w')\n",
    "    for step in range(500):\n",
    "        \n",
    "        \n",
    "        batch_images = F_data[step].reshape([1, 42])\n",
    "        batch_z = np.random.uniform(0, 1, size=[1, z_dim])\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], {X: batch_images, z: batch_z})\n",
    "        _ = sess.run(g_optimizer, feed_dict={inputs_z : batch_z})\n",
    "        \n",
    "        sample_z = np.random.uniform(0, 1, size=[1, z_dim])  # 16 Samples each epoch\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], {z: sample_z})\n",
    "        \n",
    "        D_loss_vals.append(D_loss_curr)\n",
    "        G_loss_vals.append(G_loss_curr)\n",
    "        if step%10==0:\n",
    "            print('step {} / {} Complete. D_Loss : {:0.3f}, G_Loss : {:0.3f}'.format(step+1, 10000, loss_d, loss_g))\n",
    "        \n",
    "        \n",
    "        gen_samples, _ = sess.run(model_generator(inputs_z, input_size, reuse=True), feed_dict={inputs_z : sample_z})\n",
    "        gen_dd = gen_samples.resize([1, 42])\n",
    "        gen_re=np.uniform(gen_samples,(1,42))\n",
    "        f.write(str(gen_dd))\n",
    "        samples.append(gen_dd)\n",
    "\n",
    "    print('Training Complete. ')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-219-33c21ee9081d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mm_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m499\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mm_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\numpy-1.15.0rc2-py3.5-win-amd64.egg\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \"\"\"\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "m_data[12].ndim\n",
    "samples[499].ndim\n",
    "m_data=np.vstack((m_data, samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:     0\tLoss: 4.760\tAcc: 53.46%\n",
      "step:    10\tLoss: 3.048\tAcc: 52.58%\n",
      "step:    20\tLoss: 2.528\tAcc: 50.55%\n",
      "step:    30\tLoss: 2.470\tAcc: 50.45%\n",
      "step:    40\tLoss: 2.466\tAcc: 50.55%\n",
      "step:    50\tLoss: 2.456\tAcc: 50.59%\n",
      "step:    60\tLoss: 2.432\tAcc: 50.58%\n",
      "step:    70\tLoss: 2.401\tAcc: 50.50%\n",
      "step:    80\tLoss: 2.382\tAcc: 50.51%\n",
      "step:    90\tLoss: 2.361\tAcc: 50.49%\n",
      "step:   100\tLoss: 2.339\tAcc: 50.45%\n",
      "step:   110\tLoss: 2.318\tAcc: 50.41%\n",
      "step:   120\tLoss: 2.297\tAcc: 50.35%\n",
      "step:   130\tLoss: 2.276\tAcc: 50.30%\n",
      "step:   140\tLoss: 2.267\tAcc: 50.31%\n",
      "step:   150\tLoss: 2.250\tAcc: 50.25%\n",
      "step:   160\tLoss: 2.231\tAcc: 50.20%\n",
      "step:   170\tLoss: 2.204\tAcc: 50.10%\n",
      "step:   180\tLoss: 2.181\tAcc: 50.03%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-bcfbd3d184f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx1_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my1_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m#print(\"training by gan sample\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\numpy-1.15.0rc2-py3.5-win-amd64.egg\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#xy = np.loadtxt('C:\\\\Users\\\\SANHA\\\\Desktop\\\\test.csv', delimiter=',', dtype=np.float32)\n",
    "#x_data=xy[:,0:-1]\n",
    "#y_data=xy[:,[-1]]\n",
    "#F_data=xy[:,:]\n",
    "\n",
    "\n",
    "x1_data=m_data[:,0:-1]\n",
    "y1_data=m_data[:,[-1]]\n",
    "\n",
    "\n",
    "\n",
    "nb_classes=2\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,41])\n",
    "Y=tf.placeholder(tf.int32,[None,1])\n",
    "\n",
    "Y_one_hot=tf.one_hot(Y,nb_classes)\n",
    "Y_one_hot=tf.reshape(Y_one_hot,[-1,nb_classes])\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([41,128]),name='weight1')\n",
    "b1=tf.Variable(tf.random_normal([128]),name='bias1')\n",
    "layer1=tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([128,128]),name='weight2')\n",
    "b2=tf.Variable(tf.random_normal([128]),name='bias2')\n",
    "layer2=tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3=tf.Variable(tf.random_normal([128,nb_classes]),name='weight3')\n",
    "b3=tf.Variable(tf.random_normal([nb_classes]),name='bias3')\n",
    "logits=tf.matmul(layer2,W3)+b3\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y_one_hot)\n",
    "\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "prediction=tf.argmax(hypothesis,1) #가능성을 퍼센트로~~\n",
    "correct_prediction=tf.equal(prediction,tf.arg_max(Y_one_hot,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        sess.run(optimizer,feed_dict={X:x1_data,Y:y1_data})\n",
    "        #print(\"training by gan sample\")\n",
    "        if step %10==0:\n",
    "            loss,acc=sess.run([cost,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "            print(\"step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step,loss,acc))\n",
    "  \n",
    "    tr=0\n",
    "    fa=0\n",
    "    total=0\n",
    "\n",
    "\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "            if(p==int(y)):\n",
    "                tr=tr+1\n",
    "            else:\n",
    "                fa=fa+1\n",
    "            #print(\"[{}] Prediction: {} Real Y: {}\".format(p == int(y), p, int(y)))\n",
    "\n",
    "    print(\"true={} false: {} acc: {:0.2f}%\".format(tr,fa,tr/(tr+fa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
